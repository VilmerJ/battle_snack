%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

@article{doi:10.1126/science.aar6404,
author = {David Silver  and Thomas Hubert  and Julian Schrittwieser  and Ioannis Antonoglou  and Matthew Lai  and Arthur Guez  and Marc Lanctot  and Laurent Sifre  and Dharshan Kumaran  and Thore Graepel  and Timothy Lillicrap  and Karen Simonyan  and Demis Hassabis },
title = {A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
journal = {Science},
volume = {362},
number = {6419},
pages = {1140-1144},
year = {2018},
doi = {10.1126/science.aar6404},
URL = {https://www.science.org/doi/abs/10.1126/science.aar6404},
eprint = {https://www.science.org/doi/pdf/10.1126/science.aar6404},
abstract = {Computers can beat humans at increasingly complex games, including chess and Go. However, these programs are typically constructed for a particular game, exploiting its properties, such as the symmetries of the board on which it is played. Silver et al. developed a program called AlphaZero, which taught itself to play Go, chess, and shogi (a Japanese version of chess) (see the Editorial, and the Perspective by Campbell). AlphaZero managed to beat state-of-the-art programs specializing in these three games. The ability of AlphaZero to adapt to various game rules is a notable step toward achieving a general game-playing system. Science, this issue p. 1140; see also pp. 1087 and 1118 AlphaZero teaches itself to play three different board games and beats state-of-the-art programs in each. The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.}}

@misc{chess.com_stockfish,
  title = {Stockfish - Chess Engines},
  howpublished = {\url{https://www.chess.com/terms/stockfish-chess-engine}},
  note = {Accessed: 2024-05-27}
}



@article{AuerPeter2002FAot,
abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.[PUBLICATION ABSTRACT]},
author = {Auer, Peter and Cesa-bianchi, Nicol√≤ and Fischer, Paul},
address = {Dordrecht},
copyright = {Kluwer Academic Publishers 2002},
issn = {0885-6125},
journal = {Machine learning},
language = {eng},
number = {2-3},
pages = {235-256},
publisher = {Springer Nature B.V},
title = {Finite-time Analysis of the Multiarmed Bandit Problem},
volume = {47},
year = {2002},
}


@inproceedings{kocsis2006bandit,
  title={Bandit based monte-carlo planning},
  author={Kocsis, Levente and Szepesv{\'a}ri, Csaba},
  booktitle={European conference on machine learning},
  pages={282--293},
  year={2006},
  organization={Springer}
}




@article{POWLEY201492,
title = {Information capture and reuse strategies in Monte Carlo Tree Search, with applications to games of hidden information},
journal = {Artificial Intelligence},
volume = {217},
pages = {92-116},
year = {2014},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2014.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370214001052},
author = {Edward J. Powley and Peter I. Cowling and Daniel Whitehouse},
keywords = {Game tree search, Hidden information, Information reuse, Machine learning, Monte Carlo Tree Search (MCTS), Uncertainty},
abstract = {Monte Carlo Tree Search (MCTS) has produced many breakthroughs in search-based decision-making in games and other domains. There exist many general-purpose enhancements for MCTS, which improve its efficiency and effectiveness by learning information from one part of the search space and using it to guide the search in other parts. We introduce the Information Capture And ReUse Strategy (ICARUS) framework for describing and combining such enhancements. We demonstrate the ICARUS framework's usefulness as a frame of reference for understanding existing enhancements, combining them, and designing new ones. We also use ICARUS to adapt some well-known MCTS enhancements (originally designed for games of perfect information) to handle information asymmetry between players and randomness, features which can make decision-making much more difficult. We also introduce a new enhancement designed within the ICARUS framework, EPisodic Information Capture and reuse (EPIC), designed to exploit the episodic nature of many games. Empirically we demonstrate that EPIC is stronger and more robust than existing enhancements in a variety of game domains, thus validating ICARUS as a powerful tool for enhancement design within MCTS.}
}

@book{RussellNorvigAIBook3rd,
  author        = {Stuart J. Russell and Peter Norvig},
  date-added    = {2014-08-23 20:37:06 +0000},
  date-modified = {2014-08-23 20:38:47 +0000},
  edition       = {3rd},
  number        = {ISBN 978-0-13-207148-2},
  publisher     = {Pearson Education},
  title         = {Artificial Intelligence - A Modern Approach},
  year          = {2010}
}

@article{browne2012survey,
  title={A survey of monte carlo tree search methods},
  author={Browne, Cameron B and Powley, Edward and Whitehouse, Daniel and Lucas, Simon M and Cowling, Peter I and Rohlfshagen, Philipp and Tavener, Stephen and Perez, Diego and Samothrakis, Spyridon and Colton, Simon},
  journal={IEEE Transactions on Computational Intelligence and AI in games},
  volume={4},
  number={1},
  pages={1--43},
  year={2012},
  publisher={IEEE}
}
